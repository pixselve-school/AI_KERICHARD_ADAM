{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 3 : Text classification on the Ohsumed dataset\n",
    "## 1. Data loading and preprocessing\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "276e96a7f7a63522"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-25T09:04:28.962918Z",
     "start_time": "2023-10-25T09:04:28.958253Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences   # to do padding or truncating\n",
    "import nltk\n",
    "from nltk.corpus import stopwords   # to get collection of stopwords\n",
    "import pandas as pd    # to load dataset\n",
    "import numpy as np     # for mathematic equation\n",
    "from nltk.corpus import stopwords   # to get collection of stopwords\n",
    "from tensorflow.keras.layers import Bidirectional, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # to encode text to int\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences   # to do padding or truncating\n",
    "from tensorflow.keras.models import Sequential     # the model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense # layers of the architecture\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint   # save model\n",
    "from tensorflow.keras.models import load_model   # load saved model\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/maelkerichard/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T08:54:14.346507Z",
     "start_time": "2023-10-25T08:54:14.145517Z"
    }
   },
   "id": "7e17febbfbb71bb2"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def get_info(path: str):\n",
    "    data = list(os.walk(path))[1:]\n",
    "    files = []\n",
    "    for d in data:\n",
    "        folder_name = d[0]\n",
    "        for file in d[2]:\n",
    "            files.append((folder_name.split('/')[-1], os.path.join(folder_name, file)))\n",
    "\n",
    "    d = defaultdict(int)\n",
    "    texts = defaultdict(list)\n",
    "    for (cate, file) in files:\n",
    "        with open(file, 'r') as outfile:\n",
    "            text = outfile.read()\n",
    "            texts[cate].append(text)\n",
    "            words = text_to_word_sequence(text)\n",
    "            for word in words:\n",
    "                d[word] += 1\n",
    "    words = sorted(d.items(), key=lambda x: x[1], reverse=True)\n",
    "    return (texts, words)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T08:47:50.832630Z",
     "start_time": "2023-10-25T08:47:50.801347Z"
    }
   },
   "id": "fac07068e5ee0c53"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "english_stops = set(stopwords.words('english'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T08:54:51.465343Z",
     "start_time": "2023-10-25T08:54:51.429521Z"
    }
   },
   "id": "e537872784cc2cfb"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def load_dataset(folder):\n",
    "\n",
    "    # LOAD DATA\n",
    "    path = '../ohsumed-first-20000-docs/' + folder\n",
    "    texts, words = get_info(path)\n",
    "    \n",
    "    # CREATE DATAFRAME\n",
    "    df = pd.DataFrame(columns=['category', 'article'])\n",
    "    for cate in texts:\n",
    "        for text in texts[cate]:\n",
    "            df = pd.concat([df, pd.DataFrame({'category': [cate], 'article': [text]})], ignore_index=True)\n",
    "    \n",
    "    # PRE-PROCESS DATA\n",
    "    df['article'] = df['article'].replace({'<.*?>': ''}, regex = True)          # remove html tag\n",
    "    df['article'] = df['article'].replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n",
    "    df['article'] = df['article'].apply(lambda article: [w for w in article.split() if w not in english_stops])  # remove stop words\n",
    "    df['article'] = df['article'].apply(lambda article: [w.lower() for w in article])   # lower case\n",
    "    \n",
    "    # ENCODE CATEGORY\n",
    "    df['category'] = df['category'].replace('C01', 0)\n",
    "    df['category'] = df['category'].replace('C02', 1)\n",
    "    df['category'] = df['category'].replace('C03', 2)\n",
    "    df['category'] = df['category'].replace('C04', 3)\n",
    "    df['category'] = df['category'].replace('C05', 4)\n",
    "    df['category'] = df['category'].replace('C06', 5)\n",
    "    df['category'] = df['category'].replace('C07', 6)\n",
    "    df['category'] = df['category'].replace('C08', 7)\n",
    "    df['category'] = df['category'].replace('C09', 8)\n",
    "    df['category'] = df['category'].replace('C10', 9)\n",
    "    df['category'] = df['category'].replace('C11', 10)\n",
    "    df['category'] = df['category'].replace('C12', 11)\n",
    "    df['category'] = df['category'].replace('C13', 12)\n",
    "    df['category'] = df['category'].replace('C14', 13)\n",
    "    df['category'] = df['category'].replace('C15', 14)\n",
    "    df['category'] = df['category'].replace('C16', 15)\n",
    "    df['category'] = df['category'].replace('C17', 16)\n",
    "    df['category'] = df['category'].replace('C18', 17)\n",
    "    df['category'] = df['category'].replace('C19', 18)\n",
    "    df['category'] = df['category'].replace('C20', 19)\n",
    "    df['category'] = df['category'].replace('C21', 20)\n",
    "    df['category'] = df['category'].replace('C22', 21)\n",
    "    df['category'] = df['category'].replace('C23', 22)\n",
    "\n",
    "    x_data = df['article']\n",
    "    \n",
    "    y_data = df['category']\n",
    "    return x_data, y_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T09:02:22.860718Z",
     "start_time": "2023-10-25T09:02:22.852275Z"
    }
   },
   "id": "b59aae34a6021006"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "x_train, y_train = load_dataset('training')\n",
    "x_test, y_test = load_dataset('test')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T09:02:32.420617Z",
     "start_time": "2023-10-25T09:02:23.938166Z"
    }
   },
   "id": "7f5e2be9e75a50b3"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def get_max_length():\n",
    "    review_length = []\n",
    "    for article in x_train:\n",
    "        review_length.append(len(article))\n",
    "\n",
    "    return int(np.ceil(np.mean(review_length)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T09:03:33.109228Z",
     "start_time": "2023-10-25T09:03:33.102217Z"
    }
   },
   "id": "b12a5156cf31873a"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded X Train\n",
      " [[  361   242    96 ...   119    96   354]\n",
      " [14808   668   464 ...     0     0     0]\n",
      " [ 1210    74    97 ...     0     0     0]\n",
      " ...\n",
      " [  669  1092  4038 ...  2897    21   669]\n",
      " [   39   240   132 ...     0     0     0]\n",
      " [    7  1270   254 ...   276  1859 13205]] \n",
      "\n",
      "Encoded X Test\n",
      " [[   74   837   711 ...     0     0     0]\n",
      " [ 1410   552    74 ...   552   240    45]\n",
      " [ 1037  1622    77 ...     0     0     0]\n",
      " ...\n",
      " [  128    21  1422 ...    30  3395  1536]\n",
      " [21723   470 12121 ...     0     0     0]\n",
      " [ 3106  5827   449 ...     0     0     0]] \n",
      "\n",
      "Maximum review length:  112\n"
     ]
    }
   ],
   "source": [
    "# ENCODE REVIEW\n",
    "token = Tokenizer(lower=False)    # no need lower, because already lowered the data in load_data()\n",
    "token.fit_on_texts(x_train)\n",
    "x_train = token.texts_to_sequences(x_train)\n",
    "x_test = token.texts_to_sequences(x_test)\n",
    "\n",
    "max_length = get_max_length()\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
    "x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "total_words = len(token.word_index) + 1   # add 1 because of 0 padding\n",
    "\n",
    "print('Encoded X Train\\n', x_train, '\\n')\n",
    "print('Encoded X Test\\n', x_test, '\\n')\n",
    "print('Maximum review length: ', max_length)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T09:03:48.279623Z",
     "start_time": "2023-10-25T09:03:47.634504Z"
    }
   },
   "id": "ebc7693267dd2262"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build Architecture/Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2261955d925d5c3d"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 112, 32)           905888    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                24832     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 930785 (3.55 MB)\n",
      "Trainable params: 930785 (3.55 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ARCHITECTURE\n",
    "EMBED_DIM = 32\n",
    "LSTM_OUT = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, EMBED_DIM, input_length=max_length))\n",
    "model.add(LSTM(LSTM_OUT))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T09:04:33.750452Z",
     "start_time": "2023-10-25T09:04:33.470147Z"
    }
   },
   "id": "84d1edc8d5576bc5"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    'models/LSTM.h5',\n",
    "    monitor='accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T09:05:11.123320Z",
     "start_time": "2023-10-25T09:05:11.118577Z"
    }
   },
   "id": "fad639e6e034c2cb"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "81/82 [============================>.] - ETA: 0s - loss: -187.2361 - accuracy: 0.0152\n",
      "Epoch 1: accuracy improved from -inf to 0.01514, saving model to models/LSTM.h5\n",
      "82/82 [==============================] - 5s 64ms/step - loss: -187.4698 - accuracy: 0.0151\n",
      "Epoch 2/5\n",
      " 2/82 [..............................] - ETA: 5s - loss: -228.3922 - accuracy: 0.0273"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maelkerichard/Library/Python/3.9/lib/python/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/82 [============================>.] - ETA: 0s - loss: -251.9250 - accuracy: 0.0151\n",
      "Epoch 2: accuracy did not improve from 0.01514\n",
      "82/82 [==============================] - 5s 67ms/step - loss: -252.0095 - accuracy: 0.0151\n",
      "Epoch 3/5\n",
      "81/82 [============================>.] - ETA: 0s - loss: -313.3766 - accuracy: 0.0150\n",
      "Epoch 3: accuracy did not improve from 0.01514\n",
      "82/82 [==============================] - 5s 64ms/step - loss: -313.6246 - accuracy: 0.0151\n",
      "Epoch 4/5\n",
      "81/82 [============================>.] - ETA: 0s - loss: -373.4250 - accuracy: 0.0151\n",
      "Epoch 4: accuracy did not improve from 0.01514\n",
      "82/82 [==============================] - 5s 65ms/step - loss: -373.4465 - accuracy: 0.0151\n",
      "Epoch 5/5\n",
      "81/82 [============================>.] - ETA: 0s - loss: -432.4989 - accuracy: 0.0151\n",
      "Epoch 5: accuracy did not improve from 0.01514\n",
      "82/82 [==============================] - 6s 67ms/step - loss: -432.3756 - accuracy: 0.0151\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size = 128, epochs = 5, callbacks=[checkpoint])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T09:05:39.739259Z",
     "start_time": "2023-10-25T09:05:12.621462Z"
    }
   },
   "id": "bb774e289958b05c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
