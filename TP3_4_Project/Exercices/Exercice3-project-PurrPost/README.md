<p align="center">
<img alt="logo" src=".github/docs/logo.png"/>
</p>

<h4 align="center">Cody ADAM & Mael KERICHARD</h4>
<p align="center">
   <img src="https://img.shields.io/badge/-ESIR-orange" alt="ESIR">
   <img src="https://img.shields.io/badge/-Langium-red" alt="Langium">
   <img src="https://img.shields.io/badge/-ASE-blue" alt="ASE">
</p>

## üåê Acc√®s rapide

Le projet est disponible √† l'adresse suivante : [https://ai-kerichard-adam.vercel.app](https://ai-kerichard-adam.vercel.app).

> ‚ö†Ô∏è Nous ne sommes pas responsables des contenus pr√©sents sur le site. Notre mod√®le fait tout pour filtrer les messages inappropri√©s, mais il peut arriver que certains passent √† travers les mailles du filet.

> ‚ÑπÔ∏è Il y a un petit d√©lai de chargement √† la premi√®re redaction d'un message, le temps que le mod√®le se charge.

## ü§î Le projet

C'est un clone tr√®s simpliste de X (ex-Twitter) qui permet de poster des messages sur un fil global. 

Le but de ce projet est de mettre en place un mod√®le de Machine Learning qui permet de filtrer les 
messages inappropri√©s en emp√™chant leur publication.

## üõ†Ô∏è Architecture et technologies

```mermaid
sequenceDiagram
    actor U as Utilisateur
    participant C as Client
    participant M as Mod√®le
    participant S as Serveur

    U ->> C: R√©daction d'un message
    C ->> M: V√©rification du message
    alt Message correct
        M ->> C: Classement du message comme correct
        C ->> S: Publication du message
    else Message inappropri√©
        M ->> C: Classement du message comme inappropri√©
        C ->> U: Signalement que le message est inappropri√© et qu'il ne peut pas √™tre publi√©
    end
    
```

Pour simplifier le d√©veloppement, la validation se fait c√¥t√© client. Le mod√®le est donc charg√© dans le navigateur de l'utilisateur.
Il est charg√© dans un Web Worker pour ne pas bloquer le thread principal (voir `app/worker.js`).

Nous utilisons une petite base de donn√©e SQLite pour stocker les messages. Le serveur est g√©r√© par Next.js et est d√©ploy√© sur Vercel.

Nous utilisons le mod√®le [_unitary/toxic-bert_](https://huggingface.co/unitary/toxic-bert) (via [_Xenova/toxic-bert_](https://huggingface.co/Xenova/toxic-bert) 
pour Transformers.js) qui est un mod√®le pr√©-entrain√© pour la classification de textes inappropri√©s.

## üëÄ Gestion du bias

Le mod√®le comporte du biais puisque les donn√©es ont √©t√©s annot√©s par des humains.

Ainsi, une personne peut trouver un message discriminatoire alors qu'une autre personne ne le trouvera pas.

De ce fait, nous avons ajout√© un bouton qui permet de signaler un message comme inappropri√©.
Ce message ne sera alors plus visible par les autres utilisateurs, mais sera toujours pr√©sent dans notre base de donn√©es.
Alors, nous pouvons r√©-annoter ce message pour am√©liorer le mod√®le (nous n'avons pas inclus cette fonctionnalit√© dans le projet).
