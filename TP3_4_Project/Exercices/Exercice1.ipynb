{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Introduction\n",
    "\n",
    "#### Exploring Transformers with Keras\n",
    "\n",
    "Welcome to this Jupyter Notebook, where we will embark on an exciting journey to explore and understand the Transformer architecture using Keras. This notebook is tailored for the Artificial Intelligence course at ESIR – Université Rennes, focusing on practical aspects of implementing and utilizing Transformers in natural language processing (NLP).\n",
    "\n",
    "#### Background\n",
    "\n",
    "Transformers, since their introduction in the paper “Attention Is All You Need” by Vaswani et al., have revolutionized the field of NLP. Known for their efficiency in handling sequential data and their ability to capture long-range dependencies, Transformers have set new benchmarks in a wide range of NLP tasks.\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "- Implement a Transformer block as a Keras layer.\n",
    "- Use this Transformer for text classification.\n",
    "- Compare its performance with a traditional LSTM-based approach.\n",
    "- Experiment with different hyperparameters to understand their impact.\n",
    "\n",
    "#### Dataset\n",
    "\n",
    "We will be working with the Ohsumed dataset, a collection of medical abstracts, which presents a challenging yet insightful task for text classification. This dataset differs from the usual IMDB dataset typically used in transformer examples, providing a more domain-specific challenge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Part 2: Setup and Imports\n",
    "\n",
    "First, let's set up our environment by importing the necessary libraries. We'll need TensorFlow and its high-level API, Keras, for building and training our models. The layers submodule from Keras provides us with the necessary layers to construct our Transformer model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "TensorFlow will serve as our backend for matrix operations and neural network functionalities, while Keras will offer a user-friendly interface to build and train our models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Part 3: Transformer Block Implementation\n",
    "\n",
    "#### Implementing the Transformer Block\n",
    "\n",
    "The core of the Transformer model is its unique architecture, which we will implement as a custom layer in Keras. The `TransformerBlock` class encapsulates the key components of the Transformer's architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Explanation\n",
    "\n",
    "This class is a representation of a single Transformer block, which consists of:\n",
    "\n",
    "1. **Multi-Head Attention Mechanism**: This allows the model to jointly attend to information from different representation subspaces at different positions.\n",
    "\n",
    "2. **Feed Forward Network**: A simple fully connected neural network applied to each position separately and identically.\n",
    "\n",
    "3. **Layer Normalization and Dropout**: Used for regularization and to stabilize the training process.\n",
    "\n",
    "4. **Residual Connections**: Encourage gradient flow through the network.\n",
    "\n",
    "This block can process a batch of sequences in parallel, making it highly efficient for NLP tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Data Preparation\n",
    "\n",
    "#### Loading the Ohsumed Dataset\n",
    "\n",
    "The Ohsumed dataset is a comprehensive collection of medical abstracts, making it an excellent choice for our text classification task. To load the dataset, we use TensorFlow Datasets (TFDS), which simplifies the process of downloading and preparing datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'core' from partially initialized module 'tensorflow_datasets' (most likely due to a circular import) (c:\\Users\\codya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_datasets\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\codya\\Git\\AI_KERICHARD_ADAM\\TP3_4_Project\\Exercices\\Exercice1.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/codya/Git/AI_KERICHARD_ADAM/TP3_4_Project/Exercices/Exercice1.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtfds\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/codya/Git/AI_KERICHARD_ADAM/TP3_4_Project/Exercices/Exercice1.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Load the Ohsumed dataset\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/codya/Git/AI_KERICHARD_ADAM/TP3_4_Project/Exercices/Exercice1.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m ds \u001b[39m=\u001b[39m tfds\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mhuggingface:ohsumed/ohsumed\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\codya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_datasets\\__init__.py:43\u001b[0m\n\u001b[0;32m     41\u001b[0m _TIMESTAMP_IMPORT_STARTS \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mabsl\u001b[39;00m \u001b[39mimport\u001b[39;00m logging\n\u001b[1;32m---> 43\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlogging\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_tfds_logging\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlogging\u001b[39;00m \u001b[39mimport\u001b[39;00m call_metadata \u001b[39mas\u001b[39;00m _call_metadata\n\u001b[0;32m     46\u001b[0m _metadata \u001b[39m=\u001b[39m _call_metadata\u001b[39m.\u001b[39mCallMetadata()\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'core' from partially initialized module 'tensorflow_datasets' (most likely due to a circular import) (c:\\Users\\codya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_datasets\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load the Ohsumed dataset\n",
    "ds = tfds.load('huggingface:ohsumed/ohsumed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Dataset Overview\n",
    "\n",
    "The Ohsumed dataset contains 348,566 references from MEDLINE, covering titles, abstracts, and other metadata from medical journals over five years (1987-1991). The dataset is split into 'train' (54,709 examples) and 'test' (293,855 examples) sets. The key features we will focus on are the 'title' and 'abstract', which will serve as the input, and 'mesh_terms' for the output labels.\n",
    "\n",
    "#### Preprocessing the Data\n",
    "\n",
    "Preprocessing includes tokenizing the text data and padding the sequences to a uniform length. This is crucial as the Transformer model expects inputs of the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\codya\\Git\\AI_KERICHARD_ADAM\\TP3_4_Project\\Exercices\\Exercice1.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/codya/Git/AI_KERICHARD_ADAM/TP3_4_Project/Exercices/Exercice1.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Tokenizer\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/codya/Git/AI_KERICHARD_ADAM/TP3_4_Project/Exercices/Exercice1.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m Tokenizer(num_words\u001b[39m=\u001b[39mvocab_size)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/codya/Git/AI_KERICHARD_ADAM/TP3_4_Project/Exercices/Exercice1.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m tokenizer\u001b[39m.\u001b[39mfit_on_texts(ds[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mabstract\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/codya/Git/AI_KERICHARD_ADAM/TP3_4_Project/Exercices/Exercice1.ipynb#X30sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Tokenize and pad sequences\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/codya/Git/AI_KERICHARD_ADAM/TP3_4_Project/Exercices/Exercice1.ipynb#X30sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_text\u001b[39m(text):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ds' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Parameters\n",
    "vocab_size = 20000  # considering the top 20k words\n",
    "maxlen = 200  # maximum length of the sequences\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(ds['train']['abstract'])\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "def preprocess_text(text):\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=maxlen)\n",
    "    return padded_sequences\n",
    "\n",
    "# Preprocess the dataset\n",
    "x_train = preprocess_text(ds['train']['abstract'])\n",
    "x_test = preprocess_text(ds['test']['abstract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Part 5: Model Building\n",
    "\n",
    "#### Creating the Classifier Model\n",
    "\n",
    "Now, we will use the previously defined Transformer block to build our classifier model. The model architecture is designed to process the input text, apply the Transformer block, and then classify the text into appropriate categories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2   # Number of attention heads\n",
    "ff_dim = 32     # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "# Model Architecture\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(2, activation=\"softmax\")(x)  # Adjust the output layer based on the number of classes\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Explanation\n",
    "\n",
    "In this architecture:\n",
    "\n",
    "- The `TokenAndPositionEmbedding` layer first processes the input sequences.\n",
    "- The `TransformerBlock` applies the attention mechanism and feedforward network.\n",
    "- The `GlobalAveragePooling1D` layer condenses the output from the transformer block.\n",
    "- The `Dense` layers at the end serve as a classifier on top of the transformer outputs.\n",
    "\n",
    "### Part 6: Model Compilation\n",
    "\n",
    "#### Compiling the Model\n",
    "\n",
    "After building the model, it needs to be compiled with the appropriate loss function and optimizer. Since this is a classification task, we'll use the 'sparse_categorical_crossentropy' loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", \n",
    "              loss=\"sparse_categorical_crossentropy\", \n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Explanation\n",
    "\n",
    "- **Optimizer**: 'Adam' is used for its efficiency in handling sparse gradients and adaptive learning rate capabilities.\n",
    "- **Loss Function**: 'sparse_categorical_crossentropy' is suitable for multi-class classification tasks where labels are integers.\n",
    "- **Metrics**: We are tracking 'accuracy' as it's a common metric for classification tasks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
